{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kPK2ix7tVIVE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import glob\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "import math \n",
    "import cv2 as cv\n",
    "# Setting seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "keras.utils.set_random_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RESULT_PATH = \"results_m\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1723, 0, 889, 20185, 1723, 889, 20185)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_SIZE = 24\n",
    "BATCH_SIZE = 32\n",
    "FRAME = 10\n",
    "CHANNEL = 5\n",
    "\n",
    "test_flist = [] \n",
    "val_flist = [] \n",
    "val_flist2 = [] \n",
    "\n",
    "train_flist = [] \n",
    "\n",
    "for cpath, dirs, files in os.walk('HUST-LEBW/training'):\n",
    "    if '.jpg' in ''.join(files):\n",
    "        train_flist += [ cpath + '/' + f for f in files if 'checkpoint' not in cpath]\n",
    "            \n",
    "for cpath, dirs, files in os.walk('mEbal'):\n",
    "    if '.jpg' in ''.join(files):\n",
    "        test_flist += [ cpath + '/' + f for f in files if 'checkpoint' not in cpath and np.sum(cv.imread(cpath + '/' + f)) !=0 ]\n",
    "\n",
    "for cpath, dirs, files in os.walk('HUST-LEBW/test'):\n",
    "    if  '.jpg' in ''.join(files):\n",
    "        val_flist += [ cpath + '/' + f for f in files if 'checkpoint' not in cpath]\n",
    "        \n",
    "for cpath, dirs, files in os.walk('MPB'):\n",
    "    if '.jpg' in ''.join(files):\n",
    "        test_flist += [ cpath + '/' + f for f in files if 'checkpoint' not in cpath and np.sum(cv.imread(cpath + '/' + f)) !=0 ]\n",
    "            \n",
    "random.shuffle(train_flist)\n",
    "\n",
    "test_flist = test_flist + val_flist\n",
    "\n",
    "\n",
    "train_lb = [ 1 if f.split('/')[2] == 'blink' or f.split('/')[2] == 'close' or f.split('/')[3] == 'close' else 0 for f in train_flist ]\n",
    "val_lb = [ 1 if f.split('/')[2] == 'blink' or f.split('/')[2] == 'close' or f.split('/')[3] == 'close' else 0 for f in val_flist ]\n",
    "test_lb = [ 1 if f.split('/')[2] == 'blink' or f.split('/')[2] == 'close' or f.split('/')[3] == 'close' else 0 for f in test_flist ]\n",
    "\n",
    "len(train_flist), len(val_flist2),len(val_flist),len(test_flist), len(train_lb), len(val_lb), len(test_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3qxi8ENIVIVF"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, FRAME, CHANNEL)\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 4, 3)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kFK348gIVIVH"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dc0957cf09c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_jpeg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def get_img(path, mode='train', trans = [0, 2, 1, 3]):\n",
    "    \n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "\n",
    "    if mode =='train':\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        \n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE*FRAME))\n",
    "    img = tf.reshape(img, (IMG_SIZE, FRAME,IMG_SIZE, 1))\n",
    "    img = tf.transpose(img, trans)\n",
    "    \n",
    "    img2 = (img - img[:,:,0:1, :])\n",
    "    img3 = (img - img[:,:,9:, :])\n",
    "    img4 = (img - img[:,:,4:5, :])\n",
    "    img5 = (img - img[:,:,5:6, :])\n",
    "\n",
    "    return tf.concat([img, img2, img3, img4, img5], axis=-1)\n",
    "\n",
    "def get_dataset(flist , lbs, mode = 'train', trans = [0 , 2, 1, 3]): # [1, 0, 2, 3]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices( flist )\n",
    "    dataset_lb = tf.data.Dataset.from_tensor_slices( lbs )\n",
    "    \n",
    "    dataset = dataset.map(lambda x: get_img(x, mode, trans), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = tf.data.Dataset.zip((dataset, dataset_lb))\n",
    "\n",
    "    if mode == 'train':\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)      \n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0b844fcb8f06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_steps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_flist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_steps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_flist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_steps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_flist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch = math.ceil(len(train_flist) / BATCH_SIZE)\n",
    "test_steps_per_epoch = math.ceil(len(test_flist) / BATCH_SIZE)\n",
    "val_steps_per_epoch = math.ceil(len(val_flist) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RyVm78pSVIVJ"
   },
   "source": [
    "## Video Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zrh4Ot1IVIVJ"
   },
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = tf.keras.layers.Conv3D(32, (3,3,3), padding='same', activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPool3D((2,2,1))\n",
    "        self.projection2 = tf.keras.layers.Conv3D(64, (3,3,3), padding='same', activation='relu')\n",
    "        self.pool2 = tf.keras.layers.MaxPool3D((2,2,1))\n",
    "        self.projection3 = tf.keras.layers.Conv3D(128, (3,3,3), padding='same')\n",
    "        self.pool4 = tf.keras.layers.AveragePooling3D((3,3,1), strides=(3,3,1))\n",
    "\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        \n",
    "        x = self.projection(videos)    \n",
    "        x = self.pool(x)\n",
    "        x = self.projection2(x)    \n",
    "        x = self.pool2(x)\n",
    "        x = self.projection3(x)    \n",
    "        x = self.pool4(x)\n",
    "\n",
    "        flattened_patches = self.flatten(x)\n",
    "        return flattened_patches\n",
    "\n",
    "    \n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _,  num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n",
    "    \n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    tubelet_embedder2,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches1 = tubelet_embedder(inputs[:,:,:,:, 0:1])\n",
    "    patches2 = tubelet_embedder2(inputs[:,:,:,:, 1:])\n",
    "    \n",
    "    patches = tf.keras.layers.Concatenate()([patches1, patches2])\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    atten = []\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output, attention_output_score = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=(2*embed_dim) // num_heads, dropout=0.1, \n",
    "        )(x1, x1, return_attention_scores=True)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim*2, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes)(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YtbqCVOVIVK"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hLgn83s2VIVK"
   },
   "outputs": [],
   "source": [
    "def vit(fid):\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM\n",
    "        ),\n",
    "        tubelet_embedder2=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM*2),\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset(train_flist, train_lb, mode = 'train', trans = [0 , 2, 1, 3])\n",
    "    test_dataset = get_dataset(test_flist, test_lb, mode = 'test', trans =[0 , 2, 1, 3])\n",
    "    val_dataset = get_dataset(val_flist, val_lb, mode = 'test',   trans =[0 , 2, 1, 3])\n",
    "\n",
    "    SEED = 42\n",
    "    os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "    keras.utils.set_random_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='model/VIT_{}'.format(fid),save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(train_dataset, steps_per_epoch=train_steps_per_epoch, epochs=EPOCHS, validation_data=val_dataset, validation_steps=val_steps_per_epoch, callbacks=[model_checkpoint_callback],)\n",
    "\n",
    "    best_model = keras.models.load_model('model/VIT_{}'.format(fid))\n",
    "\n",
    "    preds = tf.nn.sigmoid(best_model.predict(test_dataset)).numpy().ravel()\n",
    "    pd.DataFrame( np.array([test_flist, preds]).T, columns=['path', 'pred']).to_csv('{}/VIT_{}.csv'.format(RESULT_PATH, fid),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0297 - accuracy: 0.9884 - val_loss: 0.8062 - val_accuracy: 0.8313\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0324 - accuracy: 0.9890 - val_loss: 0.7349 - val_accuracy: 0.8301\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.8079 - val_accuracy: 0.8155\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.8682 - val_accuracy: 0.8245\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.8362 - val_accuracy: 0.8324\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0119 - accuracy: 0.9959 - val_loss: 0.8616 - val_accuracy: 0.8301\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.9364 - val_accuracy: 0.8369\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.9387 - val_accuracy: 0.8065\n",
      "Epoch 79/100\n",
      "53/54 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9982INFO:tensorflow:Assets written to: model/VIT_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/VIT_5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 3s 64ms/step - loss: 0.0041 - accuracy: 0.9983 - val_loss: 0.8700 - val_accuracy: 0.8526\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9977INFO:tensorflow:Assets written to: model/VIT_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/VIT_5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 4s 72ms/step - loss: 0.0056 - accuracy: 0.9977 - val_loss: 0.8708 - val_accuracy: 0.8594\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.9924 - val_accuracy: 0.8301\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 8.1279e-04 - accuracy: 1.0000 - val_loss: 1.0797 - val_accuracy: 0.8211\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 0.0021 - accuracy: 0.9988 - val_loss: 1.1677 - val_accuracy: 0.8110\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 8.2958e-04 - accuracy: 1.0000 - val_loss: 1.0355 - val_accuracy: 0.8391\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 4.1439e-04 - accuracy: 1.0000 - val_loss: 1.0332 - val_accuracy: 0.8414\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 2.1555e-04 - accuracy: 1.0000 - val_loss: 1.0453 - val_accuracy: 0.8346\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 2s 33ms/step - loss: 2.6041e-04 - accuracy: 1.0000 - val_loss: 1.0541 - val_accuracy: 0.8268\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 1.2262e-04 - accuracy: 1.0000 - val_loss: 1.0523 - val_accuracy: 0.8335\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 1.5489e-04 - accuracy: 1.0000 - val_loss: 1.0558 - val_accuracy: 0.8358\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 1.1119e-04 - accuracy: 1.0000 - val_loss: 1.0619 - val_accuracy: 0.8380\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 1.0797e-04 - accuracy: 1.0000 - val_loss: 1.0664 - val_accuracy: 0.8391\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 1.0028e-04 - accuracy: 1.0000 - val_loss: 1.0749 - val_accuracy: 0.8380\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 9.4810e-05 - accuracy: 1.0000 - val_loss: 1.0768 - val_accuracy: 0.8403\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 9.1045e-05 - accuracy: 1.0000 - val_loss: 1.0779 - val_accuracy: 0.8391\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 9.5045e-05 - accuracy: 1.0000 - val_loss: 1.0769 - val_accuracy: 0.8403\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 8.2005e-05 - accuracy: 1.0000 - val_loss: 1.0844 - val_accuracy: 0.8380\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 9.2202e-05 - accuracy: 1.0000 - val_loss: 1.0896 - val_accuracy: 0.8380\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 6.8106e-05 - accuracy: 1.0000 - val_loss: 1.0930 - val_accuracy: 0.8380\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 7.0784e-05 - accuracy: 1.0000 - val_loss: 1.0991 - val_accuracy: 0.8358\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 2s 35ms/step - loss: 6.6614e-05 - accuracy: 1.0000 - val_loss: 1.1019 - val_accuracy: 0.8358\n"
     ]
    }
   ],
   "source": [
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "for fid in range(1,6):\n",
    "    vit(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "vivit",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "aa",
   "language": "python",
   "name": "aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
